{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epcohraft Tutorial (beta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "```bash\n",
    "pip install epochraft\n",
    "pip install smart_open[s3] # Install S3 deps\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Construction\n",
    "\n",
    "This is an example of building a typical pretraining dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Size([8, 1024]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from epochraft import CheckpointableDataset\n",
    "from transformers import LlamaTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"NovelAI/nerdstash-tokenizer-v1\")\n",
    "\n",
    "url = \"s3://polyglot-ja-west/2_quality_filter/v2/cc-100/cc-100_00.jsonl\"\n",
    "\n",
    "dataset = (\n",
    "    CheckpointableDataset\n",
    "    .from_files(url, repeat=True, shuffle_shards=True)\n",
    "    .tokenize(tokenizer)        # Tokenize the texts\n",
    "    .ensure_bos_eos(tokenizer)  # Add BOS and EOS tokens where necessary\n",
    "    .concat_chunk(1024)         # Concatenate and chunk the tokens into a fixed length of 1024 tokens\n",
    "    .shuffle(1000)              # Shuffle the sequences using a buffer of size 1000\n",
    "    .batch(8)                   # Group the data into mini-batches with a batch size of 8\n",
    ")\n",
    "\n",
    "it = dataset.iter()\n",
    "batch = next(it)\n",
    "type(batch[\"input_ids\"]), batch[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpointing\n",
    "\n",
    "Normally, you would obtain and save the state_dict of the model and optimizer. In addition to that, please also obtain and save the state_dict of the iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = {}\n",
    "state_dict[\"it\"] = it.state_dict()\n",
    "torch.save(state_dict, \"checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumption\n",
    "\n",
    "You can restore the state of the iterator by passing the `state_dict` to the iter method of the `CheckpointableDataset` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Size([8, 1024]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(\"checkpoint.pth\")\n",
    "it = dataset.iter(state_dict=state_dict[\"it\"])\n",
    "batch = next(it)\n",
    "type(batch[\"input_ids\"]), batch[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "## Design\n",
    "Epochraft is designed to achieve the following three features:\n",
    "\n",
    "1. Streaming from Cloud Stoarge\n",
    "2. On-the-Fly Tokenization\n",
    "3. Data Loader Checkpointing\n",
    "\n",
    "To my knowledge, only epochraft offers all three of these features. For more details, please refer to the README.md at https://github.com/iwiwi/epochraft/.\n",
    "\n",
    "## Main Components\n",
    "The main class in epochraft is `CheckpointableDataset`. It is constructed as follows:\n",
    "\n",
    "* Use class methods like `from_files` to create an instance of `CheckpointableDataset`.\n",
    "* Apply transformations to the dataset by calling functions such as `tokenize` or `shuffle`.\n",
    "* Combine multiple `CheckpointableDataset` instances using methods like `interleave_datasets`.\n",
    "\n",
    "When you want to actually read data from the `CheckpointableDataset`, you call the `iter` method to obtain a `CheckpointableIterator`. This provides a standard Python iterator interface, so you can retrieve elements using the `next` function or loop through them using a `for` loop.\n",
    "\n",
    "Furthermore, by calling the `state_dict` method of the `CheckpointableIterator`, you can capture its current state. This state can be passed back to the iter method to restore the iterator's position. Epochraft is fully deterministic, ensuring an exact restoration of the state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Source\n",
    "\n",
    "## `from_files` Method\n",
    "\n",
    "To stream data from Cloud Storage, you use the `CheckpointableDataset.from_files` method. The first argument, `urls`, should be provided with either a string representing the URL or a list of such strings.\n",
    "\n",
    "Any URL that is supported by `smart_open` should work with this function. Naturally, it can also read local files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '嬉しい事に、お料理だけでなく日々のおやつもワタシのレシピを活用して下さっている方もいらっしゃるとのこと。'}\n"
     ]
    }
   ],
   "source": [
    "# A single URL\n",
    "dataset: CheckpointableDataset = CheckpointableDataset.from_files(\"s3://polyglot-ja-west/2_quality_filter/v2/cc-100/cc-100_00.jsonl\")\n",
    "sample = next(iter(dataset))\n",
    "print(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '嬉しい事に、お料理だけでなく日々のおやつもワタシのレシピを活用して下さっている方もいらっしゃるとのこと。'}\n"
     ]
    }
   ],
   "source": [
    "# A list of URLs\n",
    "dataset: CheckpointableDataset = CheckpointableDataset.from_files([\n",
    "    \"s3://polyglot-ja-west/2_quality_filter/v2/cc-100/cc-100_00.jsonl\",\n",
    "    \"s3://polyglot-ja-west/2_quality_filter/v2/cc-100/cc-100_01.jsonl\"\n",
    "])\n",
    "sample = next(iter(dataset))\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '嬉しい事に、お料理だけでなく日々のおやつもワタシのレシピを活用して下さっている方もいらっしゃるとのこと。'}\n"
     ]
    }
   ],
   "source": [
    "# `braceexpand` is automatically applied to the URL\n",
    "dataset: CheckpointableDataset = CheckpointableDataset.from_files(\n",
    "    \"s3://polyglot-ja-west/2_quality_filter/v2/cc-100/cc-100_{00..99}.jsonl\",\n",
    ")\n",
    "sample = next(iter(dataset))\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "* Currently, we support JSONL and CBOR formats. While we attempt to infer the format from the file extension, if this fails you can specify the format explicitly using the `format` argument.\n",
    "* Each sample is expected to be a `dict`. In a typical LLM (Language Model) training, it should contain a `text` field.\n",
    "* Large datasets should be split into multiple shard files. This allows for operations like shuffling and facilitates data partitioning in data-parallel training.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Arguments\n",
    "\n",
    "* The `repeat` and `shuffle_shards` are arguably the most important arguments. Typically, for a training dataset, both would be set to `True`, while for a validation dataset, both would be set to `False`.\n",
    "* `n_active_shards` specifies the number of shards that are opened and read simultaneously (called *active shards*). Samples will be alternately read from these shards.\n",
    "* `n_standby_shards` defines the number of shards that are pre-opened and pre-fetched in the background (called *standby shards*). This is used to cover the time taken to open or read files. When one of the active shards reaches its end, a standby shard becomes an active shard, and a new standby shard is opened.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Construction Methods\n",
    "\n",
    "You can also construct datasets using methods like `CheckpointableDataset.from_sequence` or `CheckpointableDataset.from_iterable`. These are particularly handy during development and debugging phases. Moreover, if you want to use HuggingFace Dataset as your data source, these methods are applicable as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '你好世界！'}\n",
      "{'text': 'こんにちは世界！'}\n",
      "{'text': '안녕하세요 세계!'}\n",
      "{'text': 'Hello world!'}\n"
     ]
    }
   ],
   "source": [
    "samples = [\n",
    "    {\"text\": \"Hello world!\"},\n",
    "    {\"text\": \"こんにちは世界！\"},\n",
    "    {\"text\": \"你好世界！\"},\n",
    "    {\"text\": \"안녕하세요 세계!\"},\n",
    "]\n",
    "\n",
    "dataset = CheckpointableDataset.from_sequence(samples, shuffle=True)\n",
    "for sample in dataset:\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Transforms: `map`, `filter`, and `filter_map`\n",
    "\n",
    "We can arbitrarily modify or filter samples by using these methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'HELLO WORLD!'}\n"
     ]
    }
   ],
   "source": [
    "def f(sample):\n",
    "    sample = sample.copy()\n",
    "    sample[\"text\"] = sample[\"text\"].upper()\n",
    "    return sample\n",
    "\n",
    "dataset = CheckpointableDataset.from_sequence(samples).map(f)\n",
    "print(next(iter(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'こんにちは世界！'}\n",
      "{'text': '你好世界！'}\n",
      "{'text': '안녕하세요 세계!'}\n"
     ]
    }
   ],
   "source": [
    "def f(sample):\n",
    "    return len(sample[\"text\"]) < 10\n",
    "\n",
    "dataset = CheckpointableDataset.from_sequence(samples).filter(f)\n",
    "for sample in dataset:\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Transforms: `parallel_map`, `parallel_filter`, and `parallel_filter_map`\n",
    "\n",
    "They are the parallel versions of `map`, `filter`, and `filter_map`, respectively. While `map`, `filter`, and `filter_map` applies the given method in the main thread, the parallel versions spawn workers and applies the method in the background.\n",
    "\n",
    "By specifying `\"thread\"` for `executor_type`, it runs in multithreading mode, while specifying `\"process\"` runs it in multiprocessing mode. The number of workers can be defined with `max_workers`.\n",
    "\n",
    "Using processes won't be limited by the GIL (Global Interpreter Lock), but there's a significant overhead in starting up workers. For tasks that aren't hampered by the GIL, it's recommended to use threads. This includes operations like IO, image decoding, and native tokenizers. Furthermore, it's advisable to keep the number of workers to the essential minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'HELLO WORLD!'}\n"
     ]
    }
   ],
   "source": [
    "def f(sample):\n",
    "    sample = sample.copy()\n",
    "    sample[\"text\"] = sample[\"text\"].upper()\n",
    "    return sample\n",
    "\n",
    "dataset = CheckpointableDataset.from_sequence(samples).parallel_map(f, executor_type=\"thread\", max_workers=1)\n",
    "print(next(iter(dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the speed is insufficient, besides increasing the values of `max_workers` and `prefetch_factor`, you can also set `ordered` to `False`. In this scenario, the dataset's order will not be preserved. While this will improve throughput, it will reduce reproducibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Using the `tokenize` method, you can apply a tokenizer to the text. The tokenizer is expected to adhere to the HuggingFace interface.\n",
    "\n",
    "You can specify the field name containing the text using `target_column` (default is `\"text\"`). Internally, it's implemented using `parallel_map`, and thus has similar arguments related to parallelization (if you set `parallel=False`, it will use `map`). Typically, tokenizers release the GIL (Global Interpreter Lock), so `executor_type` being set to `\"thread\"` should be sufficient. If the speed is inadequate, consider increasing the `max_workers` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Hello world!', 'input_ids': [2, 13071, 1190, 49338], 'attention_mask': [1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "dataset = (\n",
    "    CheckpointableDataset\n",
    "    .from_sequence(samples)\n",
    "    .tokenize(tokenizer, max_workers=2)\n",
    ")\n",
    "print(next(iter(dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOS and EOS\n",
    "\n",
    "In pretraining, documents are split or combined so that the sequence length matches the specified context length exactly (in epochraft, this process is referred to as *chunking*). Before this, it's necessary to add BOS (beginning of sentence) and EOS (end of sentence) tokens to the start and end of each document.\n",
    "\n",
    "The `ensure_bos_eos` method checks for the presence of BOS and EOS tokens at the beginning and end, and adds them only if they are absent. However, if the token IDs for BOS and EOS are the same, it won't add the other if one is already present. This ensures that the tokens don't appear consecutively when sentences are concatenated.\n",
    "\n",
    "The process handles the `target_column` field of each sample (default: `\"input_ids\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Hello world!', 'input_ids': tensor([    2, 13071,  1190, 49338,     3]), 'attention_mask': [1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "dataset = (\n",
    "    CheckpointableDataset\n",
    "    .from_sequence(samples)\n",
    "    .tokenize(tokenizer, max_workers=2)\n",
    "    .ensure_bos_eos(tokenizer)\n",
    ")\n",
    "print(next(iter(dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "\n",
    "For pretraining purposes, the original documents vary in length, but they need to be split or combined to match the model's context length exactly.\n",
    "\n",
    "There are two methods available: `chunk` and `concat_chunk`.\n",
    "\n",
    "* `chunk` splits each sample at the intervals of `chunk_length`. The remainder parts of the text shorter than `chunk_length` are discarded (this behavior can be modified using the `drop_remainder` argument). More than one document never appears in the same sequence.\n",
    "* `concat_chunk` concats remainder parts of the text shorter than `chunk_length` with the next sample. No portions are discarded in this method. More than one document may appear in the same sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([0, 1, 2, 3, 4, 5])}\n",
      "{'input_ids': tensor([20, 21, 22, 23, 24, 25])}\n"
     ]
    }
   ],
   "source": [
    "samples = [\n",
    "    {\"input_ids\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]},\n",
    "    {\"input_ids\": [10, 11, 12]},\n",
    "    {\"input_ids\": [20, 21, 22, 23, 24, 25, 26]}\n",
    "]\n",
    "\n",
    "dataset = CheckpointableDataset.from_sequence(samples).chunk(6)\n",
    "for sample in dataset:\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([0, 1, 2, 3, 4, 5])}\n",
      "{'input_ids': tensor([ 6,  7,  8,  9, 10, 11])}\n",
      "{'input_ids': tensor([12, 20, 21, 22, 23, 24])}\n"
     ]
    }
   ],
   "source": [
    "dataset = CheckpointableDataset.from_sequence(samples).concat_chunk(6)\n",
    "for sample in dataset:\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other\n",
    "\n",
    "There are several transformation functions available, with the following two being the most essential:\n",
    "\n",
    "* `batch` groups the specified number of samples into a single batch. You will likely use this function all the time.\n",
    "* `shuffle` buffers the specified number of samples and randomly samples from this buffer to partially shuffle the order. Although this isn't a full shuffle, it follows the same approach used by successful libraries such as `tf.data` and `webdataset`.\n",
    "\n",
    "Other useful functions include `take`, `stride`, and `cache`. The `cache` function is particularly handy for short and repeatedly accessed datasets, like validation datasets.\n",
    "\n",
    "In SFT (Supervised Fine-Tuning), padding is often performed instead of chunking. In such cases, you can use the `pad` function. There's also a `pack_chunk` function designed for packing, as described in the Orca paper. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combinations\n",
    "You can combine multiple `CheckpointableDataset` instances to create a single `CheckpointableDataset`.\n",
    "\n",
    "* By using `interleave_datasets`, you can sample from multiple datasets alternately. It's also possible to specify weights for each dataset. This method is primarily used for training data.\n",
    "* With `concat_datasets`, you can concatenate multiple datasets in sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Hello world!'}\n",
      "{'text': 'こんにちは世界！'}\n",
      "{'text': 'Hello world!'}\n",
      "{'text': 'こんにちは世界！'}\n",
      "{'text': 'Hello world!'}\n",
      "{'text': 'Hello world!'}\n",
      "{'text': 'こんにちは世界！'}\n",
      "{'text': 'Hello world!'}\n",
      "{'text': 'こんにちは世界！'}\n",
      "{'text': 'Hello world!'}\n"
     ]
    }
   ],
   "source": [
    "from epochraft import interleave_datasets\n",
    "\n",
    "dataset1: CheckpointableDataset = CheckpointableDataset.from_sequence([{\"text\": \"Hello world!\"}], repeat=True)\n",
    "dataset2: CheckpointableDataset = CheckpointableDataset.from_sequence([{\"text\": \"こんにちは世界！\"}], repeat=True)\n",
    "\n",
    "dataset = interleave_datasets([dataset1, dataset2], weights=[1.5, 1])\n",
    "it = iter(dataset)\n",
    "for _ in range(10):\n",
    "    print(next(it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Hello world!'}\n",
      "{'text': 'Hello world!'}\n",
      "{'text': 'Hello world!'}\n",
      "{'text': 'こんにちは世界！'}\n",
      "{'text': 'こんにちは世界！'}\n",
      "{'text': 'こんにちは世界！'}\n"
     ]
    }
   ],
   "source": [
    "from epochraft import concat_datasets\n",
    "\n",
    "dataset1: CheckpointableDataset = CheckpointableDataset.from_sequence([{\"text\": \"Hello world!\"}], repeat=True).take(3)\n",
    "dataset2: CheckpointableDataset = CheckpointableDataset.from_sequence([{\"text\": \"こんにちは世界！\"}], repeat=True).take(3)\n",
    "\n",
    "dataset = concat_datasets([dataset1, dataset2])\n",
    "for sample in dataset:\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Training\n",
    "\n",
    "When conducting data-parallel (DP) training, it's crucial that each DP worker handles different data. There are two methods to achieve this:\n",
    "\n",
    "1. Use the `stride` method of `CheckpointableDataset`. This approach uses only every `offset`-th sample modulo `interval` and discards the rest. By setting `interval` to the world size and `offset` to the rank, data can be distributed among the workers. An advantage of this method is that, even if you change the number of DP workers, the order of samples remains unchanged, ensuring high reproducibility. However, it increases the demand on the speed of dataset loading.\n",
    "\n",
    "2. Provide different URLs to `from_files` for each DP worker. For instance, each DP worker uses `urls[rank::world_size]`, where `urls` is the full URL list of the shards. The advantage here is that each DP worker reads a different file, which is efficient. However, changing the number of DP workers alters the order of the samples, and it's no longer possible to load a previously saved state_dict.\n",
    "\n",
    "Choosing between the two methods will depend on your training setup and the priorities of your workload, whether it's more crucial to maintain reproducibility or to optimize for loading speed and efficiency.\n",
    "\n",
    "TODO: write some examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: write more\n",
    "\n",
    "### Overlapping GPU Computations and Data Fetching\n",
    "\n",
    "The primary objective is to guarantee that calculations on GPUs remain on the critical path. Executing actual computations on GPUs and the enqueueing of CUDA kernels from the CPU are asynchronous. It's vital to keep the CUDA queue full to achieve optimal training performance.\n",
    "\n",
    "To ensure that data preparation doesn't slow down the training process, the specific place in the training loop to fetch the next data batch becomes pivotal. Initiating the fetch immediately after enqueueing many computationally intensive CUDA kernels ensures that the data is prepared well before the CUDA queue is exhausted. Such a strategy effectively offsets the time spent on data retrieval. Procuring data immediately after either the forward or backward pass is probably a good strategy, like the following.\n",
    "\n",
    "```python\n",
    "it = iter(dataset)\n",
    "batch = next(it)  # Fetching the first batch\n",
    "for _ in range(n_steps):\n",
    "    ...\n",
    "    loss = model(batch[\"input_ids\"])\n",
    "    batch = next(it)  # Fetching the next batch\n",
    "    loss.backward()\n",
    "    ...\n",
    "```\n",
    "\n",
    "By the way, by using pinned memory, data transfer operations between GPUs and CPUs can also be made asynchronous.\n",
    "\n",
    "\n",
    "### Reducing Fetch Time\n",
    "\n",
    "If the time taken by `next(it)` becomes longer than GPU computations, performance enhancements are needed.\n",
    "\n",
    "If data loading is slow, try increasing the `n_active_shards` value in `from_files`.\n",
    "\n",
    "Additionally, if transforms like tokenization are slow, make sure to utilize parallelism. Ensure also that the related arguments are set correctly.\n",
    "Besides increasing the values of `max_workers` and `prefetch_factor`, you can also set `ordered` to `False`. In this scenario, the dataset's order will not be maintained. Although this boosts throughput, it compromises reproducibility.\n",
    "\n",
    "For validation datasets, which use the same data repeatedly, it's advisable to use the `cache` method.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Reducing Start Time\n",
    "\n",
    "Depending on the configuration, instantiating the dataset or iterator, as well as fetching the first batch, can take a very long time. If it feels like it's taking forever, try the following solutions:\n",
    "\n",
    "* Reduce the `buffer_size` argument of the `shuffle`. Since the shuffle buffer must be filled before starting, a large buffer size can significantly slow down initialization.\n",
    "* Set the `executor_type` of parallel transforms to `\"thread\"`. Launching child processes can be slow, whereas initiating child threads is faster. Furthermore, many operations like IO and tokenization are not hampered by GIL, making threads suitable for these tasks.\n",
    "* Reduce the value of `max_workers` in parallel transforms.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with PyTorch's DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `CheckpointableDataset` class inherits from `torch.utils.data.IterableDataset`. Therefore, it can be used with the `torch.utils.data.DataLoader` if you want.\n",
    "\n",
    "However, this is *not recommended* for the following reasons:\n",
    "\n",
    "1. **Loss of Checkpointing**: The `DataLoader` class is designed to handle all data preparation using child processes. As a result, it becomes impossible to retrieve and save the `state_dict` from the main process.\n",
    "2. **It's Unnecessary**: Epochraft is designed to carry out slow operations, such as file reading and tokenization, in child threads or processes. Thus, there is no merits to use `DataLoader`.\n",
    "\n",
    "Given these reasons, while it's technically possible to use `CheckpointableDataset` with `DataLoader`, it's best to avoid doing so to make the most of its features and maintain the intended workflow.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
